{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook introduces the h2ohyperopt module for Python. Here we begin with using multi model dockers and introduce the functions available to a user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Imports\n",
    "import h2o\n",
    "import h2ohyperopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td>H2O cluster uptime: </td>\n",
       "<td>4 hours 30 minutes 55 seconds 282 milliseconds </td></tr>\n",
       "<tr><td>H2O cluster version: </td>\n",
       "<td>3.8.3.3</td></tr>\n",
       "<tr><td>H2O cluster name: </td>\n",
       "<td>H2O_started_from_python_abhishek_bdx287</td></tr>\n",
       "<tr><td>H2O cluster total nodes: </td>\n",
       "<td>1</td></tr>\n",
       "<tr><td>H2O cluster total free memory: </td>\n",
       "<td>1.11 GB</td></tr>\n",
       "<tr><td>H2O cluster total cores: </td>\n",
       "<td>4</td></tr>\n",
       "<tr><td>H2O cluster allowed cores: </td>\n",
       "<td>4</td></tr>\n",
       "<tr><td>H2O cluster healthy: </td>\n",
       "<td>True</td></tr>\n",
       "<tr><td>H2O Connection ip: </td>\n",
       "<td>127.0.0.1</td></tr>\n",
       "<tr><td>H2O Connection port: </td>\n",
       "<td>54321</td></tr>\n",
       "<tr><td>H2O Connection proxy: </td>\n",
       "<td>None</td></tr>\n",
       "<tr><td>Python Version: </td>\n",
       "<td>2.7.12</td></tr></table></div>"
      ],
      "text/plain": [
       "------------------------------  ----------------------------------------------\n",
       "H2O cluster uptime:             4 hours 30 minutes 55 seconds 282 milliseconds\n",
       "H2O cluster version:            3.8.3.3\n",
       "H2O cluster name:               H2O_started_from_python_abhishek_bdx287\n",
       "H2O cluster total nodes:        1\n",
       "H2O cluster total free memory:  1.11 GB\n",
       "H2O cluster total cores:        4\n",
       "H2O cluster allowed cores:      4\n",
       "H2O cluster healthy:            True\n",
       "H2O Connection ip:              127.0.0.1\n",
       "H2O Connection port:            54321\n",
       "H2O Connection proxy:\n",
       "Python Version:                 2.7.12\n",
       "------------------------------  ----------------------------------------------"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Initializing h2o\n",
    "h2o.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Processing\n",
    "The test dataset used for demonstrating the capabilities of H2OHyperopt is the titanic dataset. The function ```data()``` is used to preprocess the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data():\n",
    "    \"\"\"\n",
    "    Function to process the example titanic dataset.\n",
    "    Train-Valid-Test split is 60%, 20% and 20% respectively.\n",
    "    Output\n",
    "    ---------------------\n",
    "    trainFr: Training H2OFrame.\n",
    "    testFr: Test H2OFrame.\n",
    "    validFr: Validation H2OFrame.\n",
    "    predictors: List of predictor columns for the Training frame.\n",
    "    response: String defining the response column for Training frame.\n",
    "    \"\"\"\n",
    "    titanic_df = h2o.import_file(path=\"https://s3.amazonaws.com/h2o-public-test-data/smalldata/gbm_test/titanic.csv\")\n",
    "\n",
    "    # Basic preprocessing\n",
    "    # columns_to_be_used - List of columns which are used in the training/test\n",
    "    # data\n",
    "    # columns_to_factorize - List of columns with categorical variables\n",
    "    columns_to_be_used = ['pclass', 'age', 'sex', 'sibsp', 'parch', 'ticket',\n",
    "                          'embarked', 'fare', 'survived']\n",
    "    columns_to_factorize = ['pclass', 'sex', 'sibsp', 'embarked', 'survived']\n",
    "    # Factorizing the columns in the columns_to_factorize list\n",
    "    for col in columns_to_factorize:\n",
    "        titanic_df[col] = titanic_df[col].asfactor()\n",
    "    # Selecting only the columns we need\n",
    "    titanic_frame = titanic_df[columns_to_be_used]\n",
    "    trainFr, testFr, validFr = titanic_frame.split_frame([0.6, 0.2],\n",
    "                                                         seed=1234)\n",
    "    predictors = trainFr.names[:]\n",
    "    # Removing the response column from the list of predictors\n",
    "    predictors.remove('survived')\n",
    "    response = 'survived'\n",
    "    return trainFr, testFr, validFr, predictors, response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Parse Progress: [##################################################] 100%\n"
     ]
    }
   ],
   "source": [
    "trainFr, testFr, validFr, predictors, response = data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mutiple Model Type Based Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us demonstrate the ModelDocker. Since the problem is a binary classification problem, we specify the metric to AUC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Docking three types of models - GBM's, GLM's and DLE's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_gbm = h2ohyperopt.GBMOptimizer(metric='auc')\n",
    "# To use the default search space\n",
    "# model_gbm.select_optimization_parameters(\"Default\")\n",
    "# To use a combination of Default parameters and the customized parameters.\n",
    "model_gbm.select_optimization_parameters({'col_sample_rate': 'Default',\n",
    "                                          'ntrees': 200,\n",
    "                                          'learn_rate': ('uniform',(0.05, 0.2)),\n",
    "                                          'nfolds': 7})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_dle = h2ohyperopt.DLEOptimizer(metric='auc')\n",
    "# Selecting parameters to optimize on\n",
    "model_dle.select_optimization_parameters({'epsilon': 'Default',\n",
    "                                        'adaptive_rate': True,                                           'hidden': ('choice', [[10, 20], [30, 40]]),\n",
    "                                        'nfolds':7})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_glm = h2ohyperopt.GLMOptimizer(metric='auc', problemType='Classification')\n",
    "# Selecting default parameters to optimize on\n",
    "model_glm.select_optimization_parameters('Default')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the individual model optimizers are created, they are passed in a list to the ModelDocker to run multi model optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initializing a docker and optimizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We initialize the docker with three different types of models each optimized over a unique surface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "glm Model Build Progress: [##################################################] 100%\n",
      "\n",
      "glm Model Build Progress: [##################################################] 100%\n",
      "\n",
      "gbm Model Build Progress: [##################################################] 100%\n",
      "\n",
      "glm Model Build Progress: [##################################################] 100%\n",
      "\n",
      "gbm Model Build Progress: [##################################################] 100%\n",
      "\n",
      "glm Model Build Progress: [##################################################] 100%\n",
      "\n",
      "deeplearning Model Build Progress: [##################################################] 100%\n",
      "\n",
      "gbm Model Build Progress: [##################################################] 100%\n",
      "\n",
      "glm Model Build Progress: [##################################################] 100%\n",
      "\n",
      "gbm Model Build Progress: [##################################################] 100%\n",
      "\n",
      "gbm Model Build Progress: [##################################################] 100%\n",
      "\n",
      "glm Model Build Progress: [##################################################] 100%\n",
      "\n",
      "deeplearning Model Build Progress: [##################################################] 100%\n",
      "\n",
      "gbm Model Build Progress: [##################################################] 100%\n",
      "\n",
      "glm Model Build Progress: [##################################################] 100%\n",
      "\n",
      "glm Model Build Progress: [##################################################] 100%\n",
      "\n",
      "deeplearning Model Build Progress: [##################################################] 100%\n",
      "\n",
      "deeplearning Model Build Progress: [##################################################] 100%\n",
      "\n",
      "glm Model Build Progress: [##################################################] 100%\n",
      "\n",
      "gbm Model Build Progress: [##################################################] 100%\n"
     ]
    }
   ],
   "source": [
    "docker = h2ohyperopt.ModelDocker([model_dle, model_gbm, model_glm], 'auc')                                     \n",
    "docker.start_optimization(num_evals=20, trainingFr=trainFr,\n",
    "                          validationFr=validFr, response=response,                                              predictors=predictors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function ```best_in_class_ensembles``` builds an ensemble from the best model of each type. We can specify the number of models of each type we need to for our ensemble. Building best in class ensembles using 2 models from each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "deeplearning prediction Progress: [##################################################] 100%\n",
      "\n",
      "deeplearning prediction Progress: [##################################################] 100%\n",
      "\n",
      "glm prediction Progress: [##################################################] 100%\n",
      "\n",
      "glm prediction Progress: [##################################################] 100%\n",
      "\n",
      "gbm prediction Progress: [##################################################] 100%\n",
      "\n",
      "gbm prediction Progress: [##################################################] 100%\n",
      "\n",
      "gbm Model Build Progress: [##################################################] 100%\n",
      "Model Trained\n"
     ]
    }
   ],
   "source": [
    "docker.best_in_class_ensembles(numModels=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To generate the test scores on the ```ensemble_model```, we use the function ```score ensemble```. Generating the test scores,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "deeplearning prediction Progress: [##################################################] 100%\n",
      "\n",
      "deeplearning prediction Progress: [##################################################] 100%\n",
      "\n",
      "glm prediction Progress: [##################################################] 100%\n",
      "\n",
      "glm prediction Progress: [##################################################] 100%\n",
      "\n",
      "gbm prediction Progress: [##################################################] 100%\n",
      "\n",
      "gbm prediction Progress: [##################################################] 100%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7771484925331079"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docker.score_ensemble(testFr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
